# -*- coding: utf-8 -*-
"""Topic_modelling_final

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1byzSwyK5IqX8lGmM61-JcU79qnwXCdDc
"""

!pip install bertopic scikit-learn nltk umap-learn sentence-transformers langdetect pdfplumber hdbscan

!pip install --upgrade bertopic

import numpy as np
import nltk
import re
from umap import UMAP
from bertopic import BERTopic
from nltk.tokenize import word_tokenize
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.manifold import TSNE, trustworthiness
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from scipy.stats import spearmanr
from sklearn.decomposition import PCA
from sklearn.metrics.pairwise import cosine_distances
from sklearn.feature_extraction.text import CountVectorizer
from bertopic.vectorizers import ClassTfidfTransformer
#from bertopic.vectorizers import ClassTFIDF
from bertopic.representation import KeyBERTInspired
from tqdm import tqdm
import re, unicodedata, json
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from langdetect import detect
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer
import numpy as np
from tqdm import tqdm
import pdfplumber
from nltk import pos_tag
from sklearn.preprocessing import normalize
from hdbscan import HDBSCAN

import warnings
warnings.filterwarnings("ignore", category=SyntaxWarning)

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('punkt_tab')
nltk.download('averaged_perceptron_tagger')

PDF_PATH = "/content/04MC01042025636879CB4F0548F490DC312D96B7D2C1.pdf"

# -------------------------------
# 1. Extract raw text using pdfplumber
# -------------------------------

pages = []
with pdfplumber.open(PDF_PATH) as pdf:

    # Define 0-index page ranges
    selected_page_indices = (
        list(range(2, 30)) +      # pages 3â€“30
        list(range(48, 50)) +     # pages 49â€“50
        list(range(65, 68)) +     # pages 66â€“68
        list(range(69, 71))       # pages 70â€“71
    )

    for idx in selected_page_indices:
        if idx < len(pdf.pages):
            text = pdf.pages[idx].extract_text() or ""
            pages.append(text)

raw = "\n".join(pages)

# -------------------------------
# 2. Normalize Unicode + whitespace
# -------------------------------
def normalize(text):
    text = unicodedata.normalize("NFKC", text)
    text = re.sub(r"\r\n?", "\n", text)
    text = re.sub(r"[ \t]+", " ", text)
    return text.strip()

raw = normalize(raw)

# -------------------------------
# 3. Remove boilerplate / headers / footers
#    These patterns come directly from your PDF
# -------------------------------
patterns_to_remove = [
    r"Caution: RBI never sends.*",        # disclaimer footer
    r"Financial Inclusion.*Mumbai.*",     # header on page 1
    r"à¤¹à¤¿à¤‚à¤¦à¥€ à¤†à¤¸à¤¾à¤¨ à¤¹à¥ˆ.*",                  # Hindi slogan
    r"RBI/\d{4}-\d{2}/\d+.*",             # circular reference lines
    r"^\s*\d+\s*$",                       # standalone page numbers
]

for pat in patterns_to_remove:
    raw = re.sub(pat, "", raw, flags=re.I | re.M | re.S)

# -------------------------------
# 4. Fix line breaks (VERY important for PDFs)
#    If a line ends with no punctuation, join it with next line.
# -------------------------------
processed_lines = []
lines = raw.split("\n")

buffer = ""
for line in lines:
    stripped = line.strip()
    if not stripped:
        continue

    if buffer == "":
        buffer = stripped
        continue

    # If previous line doesn't end with punctuation and current starts lowercase/digit
    if (not re.search(r"[.!?]$", buffer)) and re.match(r"^[a-z0-9]", stripped):
        buffer += " " + stripped
    else:
        processed_lines.append(buffer)
        buffer = stripped

if buffer:
    processed_lines.append(buffer)

clean_text = "\n".join(processed_lines)

# -------------------------------
# 5. Sentence tokenization (NLTK)
# -------------------------------
sentences = sent_tokenize(clean_text)

def remove_bullets(s):

    # Remove bullets at the START of sentence
    s = re.sub(r"^\s*[\â€¢\-\*]\s*", "", s)                          # â€¢ bullet
    s = re.sub(r"^\s*[a-zA-Z]\s*[\.\)]\s*", "", s)                # a. , a)
    s = re.sub(r"^\s*\(?[a-zA-Z]\)\s*", "", s)                    # (a)
    s = re.sub(r"^\s*\(?[ivxIVX]+\)\s*", "", s)                   # (i), (iv)
    s = re.sub(r"^\s*[ivxIVX]+\s*[\.\)]\s*", "", s)               # i. , iv)

    # Remove bullets appearing IN THE MIDDLE of sentences
    s = re.sub(r"\s+[a-zA-Z]\s*[\.\)]\s+", " ", s)                # a. b. c.
    s = re.sub(r"\s+\(?[a-zA-Z]\)\s+", " ", s)                    # (a) (b)
    s = re.sub(r"\s+[ivxIVX]+\s*[\.\)]\s+", " ", s)               # i. ii. iii.
    s = re.sub(r"\s+\(?[ivxIVX]+\)\s+", " ", s)                   # (i) (ii)

    return s.strip()

import re
from nltk import word_tokenize, pos_tag

# -------------------------------
# CONFIGURATION FOR SENTENCE FILTERING
# -------------------------------
MIN_WORDS = 3         # minimum number of words
MIN_CHARS = 15        # minimum characters
USE_POS_VERB_CHECK = False    # make True if you want only sentences with verbs

WHITELIST_PATTERNS = [
    r"\bCD Ratio\b",
    r"\bSLBC\b",
    r"\bDCC\b",
    r"\bLDM\b",
    r"\bDBT\b",
    r"\b\d{4}\b"  # years like 2019, 2020, etc.
]

def is_whitelisted(s):
    for pat in WHITELIST_PATTERNS:
        if re.search(pat, s, flags=re.I):
            return True
    return False

def has_verb(s):
    """Check if sentence contains at least one verb."""
    try:
        tokens = word_tokenize(s)
        tags = pos_tag(tokens)
        return any(tag.startswith("VB") for _, tag in tags)
    except:
        return False

def keep_sentence(s, min_words=MIN_WORDS, min_chars=MIN_CHARS,
                  verb_check=USE_POS_VERB_CHECK):
    """Apply all rules: minimum words, characters, optional POS-verb check, whitelist."""

    s = s.strip()
    if not s:
        return False

    # Count words
    words = re.findall(r"\S+", s)

    # Word count filter
    if len(words) < min_words and not is_whitelisted(s):
        return False

    # Character length filter
    if len(s) < min_chars and not is_whitelisted(s):
        return False

    # Optional verb check
    if verb_check:
        if not has_verb(s) and not is_whitelisted(s):
            return False

    return True


# -------------------------------
# FINAL CLEANING + FILTERING LOOP
# -------------------------------
processed_sents = []

for s in sentences:
    s = s.strip()

    # Remove section numbers like 2.2.4, 1.1.3, etc.
    s = re.sub(r"^\d+(\.\d+){0,3}\s*", "", s)

    # ðŸ”¥ Remove all bullet types
    s = remove_bullets(s)

    # Collapse spaces
    s = re.sub(r"\s+", " ", s)

    # Apply advanced filter
    if keep_sentence(s):
        processed_sents.append(s)

english_sents = []

for s in processed_sents:

    # Try to detect language
    try:
        lang = detect(s)
    except:
        lang = "unknown"

    # Keep English OR whitelisted domain tokens
    if lang.startswith("en") or is_whitelisted(s):
        english_sents.append(s)

english_sents = list(set(english_sents))

# 4) minimal cleaning: remove empty lines
text_sen = [s.strip() for s in english_sents if len(s.strip()) > 0]

# 5) Basic embedding
model = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = model.encode(text_sen, convert_to_numpy=True)
print("Embeddings shape:", embeddings.shape)

pca = PCA(n_components=0.85, whiten=True, random_state=42)
embeddings_pca = pca.fit_transform(embeddings)
print("Embeddings shape after PCA:", embeddings_pca.shape)

embeddings_pca_norm = normalize(embeddings_pca, norm="l2", axis=1)
print("Embeddings shape after normalization:", embeddings_pca_norm.shape)

# 2) Explain variance summary
explained = pca.explained_variance_ratio_.sum() if 0.85 <= pca.n_components_ else pca.explained_variance_ratio_.sum()
print(f"Total explained variance by {0.85} components: {explained:.3f}")
# show first few ratios
print("Top explained variance ratios (first 10):", pca.explained_variance_ratio_[:10])

from bertopic import BERTopic
from bertopic.dimensionality import BaseDimensionalityReduction

empty_reduction_model = BaseDimensionalityReduction()

# Build a custom HDBSCAN model for BERTopic
# -----------------------
hdbscan_model = HDBSCAN(min_cluster_size=5,    # tune this: 5-20 typical
              min_samples=1,        # lower => more clusters; raise for tighter clusters
              metric='euclidean',   # using Euclidean on normalized PCA approximates cosine
              cluster_selection_method='eom',
              prediction_data=True)

vectorizer_model = CountVectorizer(
    stop_words="english",
    ngram_range=(1, 2),
    min_df=2,                 # ignore extremely rare tokens
    max_df=0.95,              # ignore tokens appearing in >95% docs
)

ctfidf_model = ClassTfidfTransformer(
    reduce_frequent_words=True,   # prevents very common tokens dominating
)

topic_model = BERTopic(
    vectorizer_model=vectorizer_model,
    ctfidf_model=ctfidf_model,
    hdbscan_model=hdbscan_model,
    embedding_model=None,
    umap_model=empty_reduction_model,
    calculate_probabilities=True,
    verbose=True
)

topics, probs = topic_model.fit_transform(
    text_sen,
    embeddings=embeddings_pca_norm
)

topic_model.get_topic_info()

new_topics = topic_model.reduce_outliers(text_sen, topics, probabilities=probs, strategy="distributions")

topic_model.update_topics(text_sen, topics=new_topics)

topic_model.get_topic_info()