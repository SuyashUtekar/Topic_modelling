{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0e3d9bc03f724130bddcadf979c5ccd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bf0f9afd19d94f2286c9757623a49ecb",
              "IPY_MODEL_783b5c29622145e3aaafd10e3ca364c4",
              "IPY_MODEL_4ccdffab2aef43d4b93c72a58625902e"
            ],
            "layout": "IPY_MODEL_82011d2b1709482da27d4831b37eae60"
          }
        },
        "bf0f9afd19d94f2286c9757623a49ecb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cec20f9171d743b18c39b72027ee20b7",
            "placeholder": "​",
            "style": "IPY_MODEL_be100dc70d9e4734a289ab3b0ec37292",
            "value": "Batches: 100%"
          }
        },
        "783b5c29622145e3aaafd10e3ca364c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf6e017e36ac423eb820f0c0264d985d",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_88d9994c3a5e4fb6a43c61e1ca356d5f",
            "value": 5
          }
        },
        "4ccdffab2aef43d4b93c72a58625902e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07318e5c40d149ebba4e497917f137e9",
            "placeholder": "​",
            "style": "IPY_MODEL_2c5412929bbb47ed98ce1042ecad431e",
            "value": " 5/5 [00:03&lt;00:00,  1.82it/s]"
          }
        },
        "82011d2b1709482da27d4831b37eae60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cec20f9171d743b18c39b72027ee20b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be100dc70d9e4734a289ab3b0ec37292": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf6e017e36ac423eb820f0c0264d985d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88d9994c3a5e4fb6a43c61e1ca356d5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "07318e5c40d149ebba4e497917f137e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c5412929bbb47ed98ce1042ecad431e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXUd3sBtcViQ"
      },
      "outputs": [],
      "source": [
        "!pip install pypdf sentence-transformers umap-learn hdbscan scikit-learn joblib bertopic[all] nltk gensim tqdm spacy\n",
        "# optionally: python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "VkVWInG5dHJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyNeppgkdBBR",
        "outputId": "77b90e8b-cd07-4d8b-a336-64e7efa2704c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# If the uploaded filename is something like \"document.pdf\":\n",
        "pdf_path = list(uploaded.keys())[0]\n",
        "pdf_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "A7kHLGQUdD9L",
        "outputId": "da359ba4-6fe0-4767-d46e-42e1e7adfe6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a12f2ec8-beb4-4d4a-a0e0-9197296be89f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a12f2ec8-beb4-4d4a-a0e0-9197296be89f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving topic_modelling.pdf to topic_modelling.pdf\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'topic_modelling.pdf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pipeline_module.py\n",
        "\n",
        "# pipeline_module.py\n",
        "import re\n",
        "import os\n",
        "from typing import List, Optional, Dict, Any, Tuple\n",
        "import joblib\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# PDF reading\n",
        "from pypdf import PdfReader\n",
        "\n",
        "# NLP\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import stopwords as nltk_stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# embedding / dimensionality reduction / clustering\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.decomposition import PCA\n",
        "from umap import UMAP\n",
        "from hdbscan import HDBSCAN\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "# BERTopic & coherence\n",
        "from bertopic import BERTopic\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from gensim.corpora import Dictionary\n",
        "\n",
        "# Ensure nltk downloads exist\n",
        "try:\n",
        "    nltk.data.find(\"tokenizers/punkt\")\n",
        "except LookupError:\n",
        "    nltk.download(\"punkt\")\n",
        "try:\n",
        "    nltk.data.find(\"corpora/stopwords\")\n",
        "except LookupError:\n",
        "    nltk.download(\"stopwords\")\n",
        "try:\n",
        "    nltk.data.find(\"corpora/wordnet\")\n",
        "except LookupError:\n",
        "    nltk.download(\"wordnet\")\n",
        "\n",
        "# Try to use spaCy if available for better lemmatization; otherwise fallback to WordNet\n",
        "USE_SPACY = False\n",
        "try:\n",
        "    import spacy\n",
        "    nlp_spacy = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
        "    USE_SPACY = True\n",
        "except Exception:\n",
        "    USE_SPACY = False\n",
        "\n",
        "# --------------------- Preprocessing utilities ---------------------\n",
        "URL_RE = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
        "\n",
        "def preprocess_text_keep_punct(text: str, lowercase: bool = True,\n",
        "                                remove_urls: bool = True,\n",
        "                                remove_stopwords: bool = True,\n",
        "                                lemmatize: bool = True,\n",
        "                                stopword_lang: str = \"english\") -> str:\n",
        "    \"\"\"\n",
        "    Preprocessing that:\n",
        "    - lowercases\n",
        "    - removes URLs\n",
        "    - removes stopwords (while keeping punctuation & numbers)\n",
        "    - lemmatizes tokens\n",
        "    - preserves punctuation tokens (they are kept as separate tokens)\n",
        "    \"\"\"\n",
        "    if remove_urls:\n",
        "        text = URL_RE.sub(\" \", text)\n",
        "    if lowercase:\n",
        "        text = text.lower()\n",
        "    # Tokenize preserving punctuation as separate tokens: words or punctuation\n",
        "    # pattern matches words (alphanumeric) or punctuation\n",
        "    token_pattern = r\"\\w+|[^\\w\\s]\"\n",
        "    tokens = re.findall(token_pattern, text)\n",
        "    # Stopwords set\n",
        "    if remove_stopwords:\n",
        "        stopset = set(nltk_stopwords.words(stopword_lang))\n",
        "    else:\n",
        "        stopset = set()\n",
        "    lem = None\n",
        "    if lemmatize:\n",
        "        if USE_SPACY:\n",
        "            # spaCy pipeline expects full text; we'll lemmatize tokens via spaCy doc\n",
        "            doc = nlp_spacy(\" \".join(tokens))\n",
        "            lem_tokens = []\n",
        "            for tok in doc:\n",
        "                if tok.is_space:\n",
        "                    continue\n",
        "                # keep punctuation and numbers\n",
        "                if tok.is_punct or tok.like_num or re.fullmatch(r\"[^\\w\\s]\", tok.text):\n",
        "                    lem_tokens.append(tok.text)\n",
        "                else:\n",
        "                    w = tok.lemma_.strip()\n",
        "                    if remove_stopwords and w in stopset:\n",
        "                        continue\n",
        "                    if remove_stopwords and tok.text in stopset:\n",
        "                        continue\n",
        "                    lem_tokens.append(w if w else tok.text)\n",
        "            return \" \".join(lem_tokens)\n",
        "        else:\n",
        "            # fallback: NLTK WordNetLemmatizer\n",
        "            lem = WordNetLemmatizer()\n",
        "            out = []\n",
        "            for t in tokens:\n",
        "                # keep punctuation tokens or numeric tokens as is\n",
        "                if re.fullmatch(r\"[^\\w\\s]\", t) or re.fullmatch(r\"\\d+[\\w\\d]*\", t):\n",
        "                    out.append(t)\n",
        "                else:\n",
        "                    t_low = t.lower()\n",
        "                    if remove_stopwords and t_low in stopset:\n",
        "                        continue\n",
        "                    lem_t = lem.lemmatize(t_low)\n",
        "                    out.append(lem_t)\n",
        "            return \" \".join(out)\n",
        "    else:\n",
        "        # no lemmatization, just remove stopwords if asked\n",
        "        out = []\n",
        "        for t in tokens:\n",
        "            if re.fullmatch(r\"[^\\w\\s]\", t) or re.fullmatch(r\"\\d+[\\w\\d]*\", t):\n",
        "                out.append(t)\n",
        "            else:\n",
        "                t_low = t.lower()\n",
        "                if remove_stopwords and t_low in stopset:\n",
        "                    continue\n",
        "                out.append(t_low)\n",
        "        return \" \".join(out)\n",
        "\n",
        "# --------------------- PDF extraction (with page mapping) ---------------------\n",
        "def extract_text_and_pages(pdf_path: str) -> Tuple[str, List[Tuple[int, str]]]:\n",
        "    \"\"\"\n",
        "    Returns full_text and a list of (page_number, page_text)\n",
        "    \"\"\"\n",
        "    reader = PdfReader(pdf_path)\n",
        "    pages = []\n",
        "    full = []\n",
        "    for i, p in enumerate(reader.pages):\n",
        "        try:\n",
        "            text = p.extract_text() or \"\"\n",
        "        except Exception:\n",
        "            text = \"\"\n",
        "        pages.append((i + 1, text))\n",
        "        full.append(text)\n",
        "    return \"\\n\".join(full), pages\n",
        "\n",
        "# --------------------- sentence tokenize with page mapping ---------------------\n",
        "def sentences_from_pages(pages: List[Tuple[int, str]]) -> Tuple[List[str], List[int]]:\n",
        "    \"\"\"\n",
        "    Returns list of sentences and list of corresponding page numbers (aligned).\n",
        "    Uses nltk.sent_tokenize.\n",
        "    \"\"\"\n",
        "    sentences = []\n",
        "    page_nums = []\n",
        "    for page_num, page_text in pages:\n",
        "        if not page_text or not page_text.strip():\n",
        "            continue\n",
        "        sents = sent_tokenize(page_text)\n",
        "        for s in sents:\n",
        "            s_clean = s.strip()\n",
        "            if s_clean:\n",
        "                sentences.append(s_clean)\n",
        "                page_nums.append(page_num)\n",
        "    return sentences, page_nums\n",
        "\n",
        "# --------------------- Embedding ---------------------\n",
        "def compute_embeddings(chunks: List[str], model_name: str = \"all-miniLM-L6-v2\", normalize_emb: bool = True):\n",
        "    model = SentenceTransformer(model_name)\n",
        "    emb = model.encode(chunks, convert_to_numpy=True, show_progress_bar=True)\n",
        "    if normalize_emb:\n",
        "        emb = normalize(emb, axis=1)\n",
        "    return emb\n",
        "\n",
        "# --------------------- Validation metrics ---------------------\n",
        "def intra_cluster_cosine(embeddings: np.ndarray, labels: np.ndarray) -> Dict[int, float]:\n",
        "    res = {}\n",
        "    unique = [u for u in set(labels) if u != -1]\n",
        "    for u in unique:\n",
        "        idx = np.where(labels == u)[0]\n",
        "        if len(idx) < 2:\n",
        "            res[u] = float(\"nan\")\n",
        "            continue\n",
        "        sims = cosine_similarity(embeddings[idx])\n",
        "        upper = sims[np.triu_indices_from(sims, k=1)]\n",
        "        res[u] = float(np.mean(upper))\n",
        "    return res\n",
        "\n",
        "def knn_overlap_score(embeddings: np.ndarray, labels: np.ndarray, k: int = 10) -> float:\n",
        "    nn = NearestNeighbors(n_neighbors=min(k, embeddings.shape[0]-1)).fit(embeddings)\n",
        "    neigh = nn.kneighbors(embeddings, return_distance=False)\n",
        "    overlaps = []\n",
        "    for i, nbs in enumerate(neigh):\n",
        "        same = set(np.where(labels == labels[i])[0])\n",
        "        if len(same) <= 1:\n",
        "            overlaps.append(0.0)\n",
        "            continue\n",
        "        overlap = len(set(nbs).intersection(same)) / float(len(nbs))\n",
        "        overlaps.append(overlap)\n",
        "    return float(np.mean(overlaps))\n",
        "\n",
        "def topic_coherence_cv(topic_model: BERTopic, chunks: List[str]) -> float:\n",
        "    # Prepare tokenized texts for gensim coherence\n",
        "    tokenized = [re.findall(r\"\\w+|[^\\w\\s]\", c.lower()) for c in chunks]  # simple tokenization preserving punctuation\n",
        "    topics = topic_model.get_topics()\n",
        "    # prepare list of topic word lists (only top words)\n",
        "    topic_word_lists = []\n",
        "    for tid, words in topics.items():\n",
        "        if tid == -1:\n",
        "            continue\n",
        "        topic_word_lists.append([w for w, _ in words])\n",
        "    if not topic_word_lists:\n",
        "        return float(\"nan\")\n",
        "    # gensim requires dictionary\n",
        "    dictionary = Dictionary(tokenized)\n",
        "    cm = CoherenceModel(topics=topic_word_lists, texts=tokenized, dictionary=dictionary, coherence='c_v')\n",
        "    return float(cm.get_coherence())\n",
        "\n",
        "def silhouette_umap(umap_embeddings: np.ndarray, labels: np.ndarray) -> Optional[float]:\n",
        "    try:\n",
        "        if len(set(labels)) <= 1:\n",
        "            return None\n",
        "        return float(silhouette_score(umap_embeddings, labels, metric=\"euclidean\"))\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def pdf_page_consistency(labels: np.ndarray, page_nums: List[int]) -> Dict[int, float]:\n",
        "    \"\"\"\n",
        "    For each cluster compute average pairwise page-distance (lower is better => cluster localized).\n",
        "    Returns mean page-span per cluster.\n",
        "    \"\"\"\n",
        "    res = {}\n",
        "    unique = [u for u in set(labels) if u != -1]\n",
        "    pages_arr = np.array(page_nums)\n",
        "    for u in unique:\n",
        "        idx = np.where(labels == u)[0]\n",
        "        if len(idx) < 2:\n",
        "            res[u] = 0.0\n",
        "            continue\n",
        "        cluster_pages = pages_arr[idx]\n",
        "        # average absolute difference between pages\n",
        "        diffs = []\n",
        "        for i in range(len(cluster_pages)):\n",
        "            for j in range(i+1, len(cluster_pages)):\n",
        "                diffs.append(abs(cluster_pages[i] - cluster_pages[j]))\n",
        "        res[u] = float(np.mean(diffs)) if diffs else 0.0\n",
        "    return res\n",
        "\n",
        "# --------------------- Main pipeline function ---------------------\n",
        "def process_pdf_pipeline(pdf_path: str,\n",
        "                         expected_num_topics: Optional[int] = None,\n",
        "                         chunking_method: str = \"sentences\",\n",
        "                         chunk_params: Optional[Dict[str, Any]] = None,\n",
        "                         pca_components: Optional[float] = 0.95,\n",
        "                         umap_components: int = 15,\n",
        "                         umap_neighbors: int = 15,\n",
        "                         hdb_min_cluster_size: int = 8,\n",
        "                         hdb_min_samples: Optional[int] = None,\n",
        "                         save_artifacts_dir: str = \"artifacts\") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Runs the full pipeline described by the user.\n",
        "    Returns a dictionary with chunks, embeddings, models, labels, topics, probs, topic_keywords, and validation metrics.\n",
        "    \"\"\"\n",
        "    os.makedirs(save_artifacts_dir, exist_ok=True)\n",
        "\n",
        "    # Step 0: extract text & pages\n",
        "    full_text, pages = extract_text_and_pages(pdf_path)\n",
        "    sentences, page_nums = sentences_from_pages(pages)\n",
        "\n",
        "    # Optional: allow other chunking later (only sentences implemented)\n",
        "    if chunking_method != \"sentences\":\n",
        "        raise NotImplementedError(\"Only 'sentences' chunking_method is implemented in this function.\")\n",
        "\n",
        "    # Step 1: Preprocess each sentence (lowercasing, stopword removal, lemmatization, keep punctuation/numbers, remove URLs)\n",
        "    preprocessed = []\n",
        "    for s in sentences:\n",
        "        preprocessed.append(preprocess_text_keep_punct(s,\n",
        "                                                      lowercase=True,\n",
        "                                                      remove_urls=True,\n",
        "                                                      remove_stopwords=True,\n",
        "                                                      lemmatize=True))\n",
        "\n",
        "    # Save raw chunks + pages mapping\n",
        "    joblib.dump((preprocessed, page_nums), os.path.join(save_artifacts_dir, \"chunks_and_pages.joblib\"))\n",
        "\n",
        "    # Step 2: Embedding using all-MiniLM-L6-v2\n",
        "    orig_emb = compute_embeddings(preprocessed, model_name=\"all-miniLM-L6-v2\", normalize_emb=True)\n",
        "    joblib.dump(orig_emb, os.path.join(save_artifacts_dir, \"orig_embeddings.joblib\"))\n",
        "\n",
        "    # Step 3: PCA\n",
        "    if pca_components is not None:\n",
        "        pca = PCA(n_components=pca_components, random_state=42)\n",
        "        pca_emb = pca.fit_transform(orig_emb)\n",
        "        joblib.dump(pca, os.path.join(save_artifacts_dir, \"pca.joblib\"))\n",
        "    else:\n",
        "        pca = None\n",
        "        pca_emb = orig_emb\n",
        "\n",
        "    # Step 4: UMAP (for clustering) - not 2D\n",
        "    umap = UMAP(n_components=umap_components, n_neighbors=umap_neighbors,\n",
        "                min_dist=0.0, metric=\"cosine\", random_state=42)\n",
        "    umap_emb = umap.fit_transform(pca_emb)\n",
        "    joblib.dump(umap, os.path.join(save_artifacts_dir, \"umap.joblib\"))\n",
        "\n",
        "    # Step 5: HDBSCAN initialization & clustering\n",
        "    hdb = HDBSCAN(min_cluster_size=hdb_min_cluster_size,\n",
        "                  min_samples=hdb_min_samples,\n",
        "                  metric=\"euclidean\",\n",
        "                  cluster_selection_method=\"eom\",\n",
        "                  prediction_data=True)\n",
        "    labels = hdb.fit_predict(umap_emb)\n",
        "    joblib.dump(hdb, os.path.join(save_artifacts_dir, \"hdbscan.joblib\"))\n",
        "\n",
        "    # Step 6: BERTopic using important params set, others None\n",
        "    topic_model = BERTopic(\n",
        "        embedding_model=None,\n",
        "        umap_model=None,\n",
        "        hdbscan_model=hdb,\n",
        "        vectorizer_model=None,\n",
        "        representation_model=None,\n",
        "        nr_topics=None,\n",
        "        calculate_probabilities=True,\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    topics, probs = topic_model.fit_transform(preprocessed, embeddings=umap_emb)\n",
        "    topic_model.save(os.path.join(save_artifacts_dir, \"bertopic_model\"))\n",
        "    # gather topic keywords\n",
        "    topics_info = {}\n",
        "    for tid in set(topics):\n",
        "        if tid == -1:  # noise\n",
        "            continue\n",
        "        topics_info[tid] = topic_model.get_topic(tid)\n",
        "\n",
        "    # Validation metrics\n",
        "    intra_cos = intra_cluster_cosine(orig_emb, np.array(topics))\n",
        "    knn_overlap = knn_overlap_score(orig_emb, np.array(topics), k=10)\n",
        "    coherence_cv = topic_coherence_cv(topic_model, preprocessed)\n",
        "    sil = silhouette_umap(umap_emb, np.array(topics))\n",
        "    page_consistency = pdf_page_consistency(np.array(topics), page_nums)\n",
        "\n",
        "    # Test expected topic count if provided\n",
        "    generated_topic_count = len([t for t in set(topics) if t != -1])\n",
        "    expected_match = None\n",
        "    if expected_num_topics is not None:\n",
        "        expected_match = (generated_topic_count == expected_num_topics)\n",
        "\n",
        "    # Merge chunks by topic -> topic-based chunking (concatenate sentence chunks belonging to same topic,\n",
        "    # optionally preserving page-order)\n",
        "    topic_chunks = {}\n",
        "    for i, t in enumerate(topics):\n",
        "        if t == -1:\n",
        "            # treat noise as its own small topics or skip; here we include them under 'noise'\n",
        "            topic_chunks.setdefault(\"noise\", []).append(preprocessed[i])\n",
        "        else:\n",
        "            topic_chunks.setdefault(int(t), []).append(preprocessed[i])\n",
        "\n",
        "    # Optionally concatenate each topic's chunks into a larger chunk for RAG context\n",
        "    merged_topic_chunks = {k: \" \".join(v) for k, v in topic_chunks.items()}\n",
        "\n",
        "    # Save artifacts & outputs\n",
        "    joblib.dump({\n",
        "        \"chunks\": preprocessed,\n",
        "        \"page_nums\": page_nums,\n",
        "        \"orig_emb\": orig_emb,\n",
        "        \"pca_emb\": pca_emb,\n",
        "        \"umap_emb\": umap_emb,\n",
        "        \"labels\": topics,\n",
        "        \"probs\": probs\n",
        "    }, os.path.join(save_artifacts_dir, \"pipeline_outputs.joblib\"))\n",
        "\n",
        "    result = {\n",
        "        \"chunks\": preprocessed,\n",
        "        \"page_nums\": page_nums,\n",
        "        \"orig_embeddings\": orig_emb,\n",
        "        \"pca_model\": pca,\n",
        "        \"pca_embeddings\": pca_emb,\n",
        "        \"umap_model\": umap,\n",
        "        \"umap_embeddings\": umap_emb,\n",
        "        \"hdbscan_model\": hdb,\n",
        "        \"topic_model\": topic_model,\n",
        "        \"topics\": topics,\n",
        "        \"probs\": probs,\n",
        "        \"topic_keywords\": topics_info,\n",
        "        \"validation\": {\n",
        "            \"intra_cluster_cosine\": intra_cos,\n",
        "            \"knn_overlap\": knn_overlap,\n",
        "            \"coherence_cv\": coherence_cv,\n",
        "            \"silhouette_umap\": sil,\n",
        "            \"page_consistency\": page_consistency\n",
        "        },\n",
        "        \"generated_topic_count\": generated_topic_count,\n",
        "        \"expected_topic_count_match\": expected_match,\n",
        "        \"merged_topic_chunks\": merged_topic_chunks,\n",
        "        \"artifacts_dir\": os.path.abspath(save_artifacts_dir)\n",
        "    }\n",
        "\n",
        "    return result\n",
        "\n",
        "# --------------------- Example usage ---------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Simple example: replace 'input.pdf' with your file and optionally set expected_num_topics\n",
        "    out = process_pdf_pipeline(\"topic_modelling.pdf\", expected_num_topics=12,\n",
        "                               hdb_min_cluster_size=8, umap_components=15)\n",
        "    print(\"Artifacts saved in:\", out[\"artifacts_dir\"])\n",
        "    print(\"Generated topic count:\", out[\"generated_topic_count\"])\n",
        "    print(\"Expected match?:\", out[\"expected_topic_count_match\"])\n",
        "    print(\"Top topic keywords (sample):\")\n",
        "    for tid, kw in list(out[\"topic_keywords\"].items())[:6]:\n",
        "        print(tid, kw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGXEyV9qcljO",
        "outputId": "0916c5b9-8352-436a-9d8e-88c19d90e040"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting pipeline_module.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pipeline_module import process_pdf_pipeline\n",
        "\n",
        "result = process_pdf_pipeline(pdf_path,\n",
        "                              expected_num_topics=5,  # optional\n",
        "                              hdb_min_cluster_size=8,\n",
        "                              umap_components=15)\n",
        "\n",
        "result[\"generated_topic_count\"], result[\"topic_keywords\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381,
          "referenced_widgets": [
            "0e3d9bc03f724130bddcadf979c5ccd0",
            "bf0f9afd19d94f2286c9757623a49ecb",
            "783b5c29622145e3aaafd10e3ca364c4",
            "4ccdffab2aef43d4b93c72a58625902e",
            "82011d2b1709482da27d4831b37eae60",
            "cec20f9171d743b18c39b72027ee20b7",
            "be100dc70d9e4734a289ab3b0ec37292",
            "bf6e017e36ac423eb820f0c0264d985d",
            "88d9994c3a5e4fb6a43c61e1ca356d5f",
            "07318e5c40d149ebba4e497917f137e9",
            "2c5412929bbb47ed98ce1042ecad431e"
          ]
        },
        "id": "JerczKzce3Op",
        "outputId": "5f147b42-753e-4a53-b5a0-bcc9b7cec102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0e3d9bc03f724130bddcadf979c5ccd0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "BERTopic.fit_transform() got an unexpected keyword argument 'umap_embeddings'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4064263880.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpipeline_module\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprocess_pdf_pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m result = process_pdf_pipeline(pdf_path,\n\u001b[0m\u001b[1;32m      4\u001b[0m                               \u001b[0mexpected_num_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# optional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                               \u001b[0mhdb_min_cluster_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/pipeline_module.py\u001b[0m in \u001b[0;36mprocess_pdf_pipeline\u001b[0;34m(pdf_path, expected_num_topics, chunking_method, chunk_params, pca_components, umap_components, umap_neighbors, hdb_min_cluster_size, hdb_min_samples, save_artifacts_dir)\u001b[0m\n\u001b[1;32m    328\u001b[0m     )\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m     \u001b[0mtopics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopic_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mumap_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m     \u001b[0mtopic_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_artifacts_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bertopic_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;31m# gather topic keywords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: BERTopic.fit_transform() got an unexpected keyword argument 'umap_embeddings'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cl4m6CQpfEbU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}